{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Машинное обучение \n",
    "### Факультет математики НИУ ВШЭ, 2021-22 учебный год\n",
    "\n",
    "[Страница курса](http://wiki.cs.hse.ru/Машинное_обучение_на_матфаке_2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание №4\n",
    "\n",
    "Фамилия и имя студента: _(впишите свои)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 1 (12 баллов)\n",
    "Пусть дана выборка $x_1, \\ldots, x_n$, все $x_i \\in \\mathbb R$ распределены как случайная величина $X$ и независимы в совокупности. $\\mathbb E[X]<\\infty$, $\\mathbb D[X]<\\infty$. Для фиксированного вектора $w\\in \\mathbb R^n$ рассмотрим функцию\n",
    "$$\\varphi_w(x)=\\langle w, x \\rangle,$$\n",
    "где $\\langle w, x \\rangle$ — стандартное скалярное произведение (скалярное произведение, записанное в ортонормированном базисе).\n",
    "\n",
    "1. При каком условии на $w$ эта функция будет несмещённой оценкой для $\\mathbb E[X]$?\n",
    "2. Среди всех $w$, при которых $\\varphi_w(x)$ является несмещённой оценкой для $\\mathbb E[X]$, найти такое, при котором дисперсия $\\varphi_w(x)$ будет наименьшей. (Подсказка: вам понадобятся множители Лагранжа.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(впишите решение сюда)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 2 (10 баллов)\n",
    "Рассмотрим задачу регрессии с одномерным пространством признаков. Пусть\n",
    "истинный закон генерирования данных описывается следующим образом: все $x_i$\n",
    "фиксированы и заданы так: $x_i=i-3$, $i=1, \\ldots, 5$, а $y_i$ являются\n",
    "случайными величинами:\n",
    "$$y_i = |x_i| + \\varepsilon_i,$$ где все $\\varepsilon_i$ независимы, $\\mathbb\n",
    "E[\\varepsilon_i]=0$, $\\mathbb D[\\varepsilon_i]=4$. Пусть $f_k(x)$\n",
    "— предсказание метода $k$ ближайших соседей в точке $x$. Найти ожидаемую\n",
    "квадратичную ошибку предсказания в точке $x=0$ для $k=3$.\n",
    "Представить её в виде суммы шума, смещения и разброса.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(впишите решение сюда)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 3 (15 баллов)\n",
    "\n",
    "Пусть числа $y_1, \\ldots, y_n$ получены как выборка из нормального распределения $\\mathcal N(\\mu, \\sigma^2)$ с неизвестными параметрами $\\mu$ и $\\sigma^2$. Мы хотим найти оценку наибольшего правдоподобия для $\\mu$ и $\\sigma^2$, то есть такие значения этих параметров, при которых функция правдоподобия\n",
    "\n",
    "$$p(y_1, \\ldots, y_n \\mid \\mu, \\sigma^2)$$\n",
    "\n",
    "будет максимальной. На лекциях была найдена функция правдоподобия и показано, что оптимальное $\\mu$ можно найти независимо от $\\sigma^2$ и оно равно выборочному среднему. \n",
    "\n",
    "1. Завершите нахождение оценки наибольшего правдоподобия: найдите теперь оптимальное $\\sigma^2$. \n",
    "\n",
    "2. Является ли полученная оценка несмещённой? Докажите."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(впишите решение сюда)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 4 (20 баллов)\n",
    "Рассмотрим следующую модель. Значения $x_1, \\ldots, x_n \\in \\mathbb R^d$ фиксированы. Вектор $w \\in \\mathbb R^d$ фиксирован. Также фиксирован вектор $\\sigma = (\\sigma_1, \\ldots, \\sigma_n) \\in \\mathbb R^n$. Значения $y_i$ определяются следующим образом:\n",
    "\n",
    "$$\\newcommand{\\eps}{\\varepsilon}y_i = \\langle w, x_i \\rangle + \\eps_i,$$\n",
    "\n",
    "где $\\eps_i$ — независимые случайные величины, распределённые по нормальному закону, $\\eps_i \\sim \\mathcal N(0, \\sigma_i^2)$ (то есть у каждого $\\eps_i$ своя дисперсия, равная $\\sigma_i^2$, все $\\sigma_i$ фиксированы и известны).\n",
    "\n",
    "1. Найти функцию правдоподобия $p((y_1, \\ldots, y_n) \\mid w, x, \\sigma)$, равную плотности вероятности получения данных $y_1, \\ldots, y_n$ при заданных фиксированных $w$, $x$ и $\\sigma$.\n",
    "2. Найти логарифм правдоподобия.\n",
    "3. Записать задачу максимизации логарфима правдоподобия по $w$. Выкинуть лишние слагаемые и записать аналог $RSS$ для этой задачи. \n",
    "4. Записать задачу максимизации правдоподобия в матричном виде. Для этого ввести матрицы $X$ (матрица объект-признак, по строкам записаны $x_1, \\ldots, x_n$) и $\\Sigma$ — диагональная матрица, у которой на диагонали стоят $\\sigma_1, \\ldots, \\sigma_n$.\n",
    "5. Решить эту задачу в матричном виде. (Найти градиент аналога $RSS$ в матричном виде, приравнять нулю, решить получившееся уравнение. Найти гессиан, показать, что он отрицательно определён в точке максимума.)\n",
    "6. Является ли полученная оценка для $w$ несмещённой?\n",
    "7. Найти ковариационную матрицу для оценки $w$.\n",
    "\n",
    "**Подсказка.** Для самопроверки можеет подставить в качестве вектора $\\sigma$ постоянный вектор (все компоненты равны одному и тому же числу). Должны получиться формулы, которые доказывались на лекциях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 5 (10 баллов)\n",
    "Маша, Неля и Катя решают задачу линейной регрессии. Данные у них одинаковые, в них $n$ наблюдений и два признака $x^{(1)}$ и $x^{(2)}$, а также вектор ответов $y$. Признаки имеют нулевое выборочное среднее и нулевую [выборочную ковариацию](https://ru.wikipedia.org/wiki/Ковариация#Ковариация_выборок). Маша находит вектор весов $(w^{M}_1, w^{M}_2)$ как МНК-оценку для задачи $y_i=w_1 x^{(1)}_i+w_2 x^{(2)}_i+\\eps_i$. Неля решила выбросить второй признак и находит вес $w^{H}_1$ как МНК-оценку для задачи $y_i=w_1 x^{(1)}_i + \\eps_i$. Катя выбросила первый признак и находит вес $w^{K}_2$ как МНК-оценку для задачи $y_i = w_2 x^{(2)}_i + \\eps_i$. Докажите, что $w^{M}_1 = w^{H}_1$ и $w^{K}_2 = w^{M}_2$. Будет ли это верно в случае, если признаки будут по-прежнему иметь нулевое среднее, но окажутся скоррелированными (то есть не будут иметь нулевую ковариацию)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 6 (10 баллов)\n",
    "У рассеянного Александра есть вектор ответов $(y_1, \\ldots, y_n)$ для задачи классификации, каждый $y_i\\in \\{0, 1\\}$, $n$ нечётное число, всего среди $y_i$ есть $m$ единиц и $(n-m)$ нулей. Александр потерял матрицу признаков, поэтому вынужден использовать алгоритм, обучающийся только по ответам, и везде предсказывающий одно и то же значение. Он хочет, чтобы алгоритм предсказывал вероятность $p$ получения единицы, и думает, какую функцию потерь ему выбрать из двух возможных:\n",
    "\n",
    "1. Log-loss: $$L_{LL}(y, p)=\\begin{cases}\n",
    "-\\log p,&\\text{ if }y=1;\\\\\n",
    "-\\log(1-p),&\\text{ if }y=0.\n",
    "\\end{cases}$$\n",
    "\n",
    "2. Абсолютное отклонение: $$L_{AD}(y, p)=|y-p|.$$\n",
    "\n",
    "Для нахождения оптимального $p$ Александр решает задачу минимизиации эмпирического риска:\n",
    "\n",
    "$$\\sum_{i=1}^n L(y_i, p) \\to \\min_{p},$$\n",
    "где $L$ — это либо $L_{LL}$, либо $L_{AD}$.\n",
    "\n",
    "Какое $p$ получится у Александра для каждой из данных функций потерь? Какую из функций потерь следует использовать, если Александр хочет, чтобы $p$ была состоятельной оценкой для вероятности получения единицы?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 7 (5 баллов)\n",
    "Кларисса решает задачу двухлассовой классификации (классы обозначаются $+1$ и $-1$) с помощью алгоритма машинного обучения, который для $i$-го объекта выдаёт степень уверенности $s_i$ алгоритма в том, что этот объект принадлежит к классу $+1$ (например, это может быть оценка вероятности, данная логистической регрессией). Кларисса выбирает пороговое значение $t$, после чего все объекты, для которых $s_i>t$, относит к положительному классу, а остальные — к отрицательному. Иными словами, окончательное предсказание классификатора имеет вид:\n",
    "$$\\hat y_i = [s_i>t] - [s_i\\le t].$$\n",
    "\n",
    "В таблице для каждого элемента обучающей выборки даны значения $s_i$ и их истинные классы. Построить ROC-кривую для данного алгоритма. (Вы можете сделать это вручную — это хорошее упражнение (посмотрите пример [здесь](https://github.com/esokolov/ml-course-hse/blob/master/2019-fall/seminars/sem05-linclass-metrics.pdf)) — либо самостоятельно написать код, который строит ROC-кривую. Использовать готовые решения из библиотек типа scikit-learn нельзя.)\n",
    "\n",
    "$$\\begin{array}{|c|c|}\n",
    "\\hline\n",
    "s_i & y_i \\\\\n",
    "\\hline\n",
    "0.6 & +1\\\\\n",
    "0.5 & -1\\\\\n",
    "0.1 & -1\\\\\n",
    "0.2 & -1\\\\\n",
    "0.4 & +1\\\\\n",
    "0.7 & +1\\\\\n",
    "\\hline\n",
    "\\end{array}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
